{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "south-friday",
   "metadata": {},
   "source": [
    "# Fine-tuning a BERT language model for Sensitive Information Detection\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Load training dataset with cudf\n",
    "* Transform labels into pytorch tensor using dlpack\n",
    "* Transform text using cudf subword tokenizer\n",
    "* Split into train and test sets\n",
    "* Loading pretrained model\n",
    "* Fine-tune the model\n",
    "* Model evaluation\n",
    "* Save model file\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Detecting sensitive information inside of text data is an arduous task, often requiring complex regex and heuristics. This notebook illustrates how to train a language model using a small sample dataset of API responses that have been previously labeled as containing up to ten different types of sensitive information. We will fine-tune a pretrained BERT model from [HuggingFace](https://github.com/huggingface) with a multi-label classification layer. We will save this model file for deployment using the Morpheus framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wrapped-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import s3fs\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "from sklearn.metrics import f1_score, accuracy_score, multilabel_confusion_matrix\n",
    "from tqdm import trange\n",
    "import cudf\n",
    "import cupy\n",
    "from cudf.utils.hash_vocab_utils import hash_vocab\n",
    "from cudf.core.subword_tokenizer import SubwordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-organic",
   "metadata": {},
   "source": [
    "## Load training dataset with cudf\n",
    "\n",
    "To train our model we begin with a dataframe containing a field with text samples and one column for each of ten labels of sensitive data. The label columns are True or False for the presence of the specific sensitive information type in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb0ac14-b976-46ee-a156-50c4139757bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cudf.read_csv(\"../datasets/training-data/sid-sample-training-data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-cathedral",
   "metadata": {},
   "source": [
    "## Transform labels into pytorch tensor using dlpack\n",
    "\n",
    "We find all the columns from the df that are labels for the text data and transform them into a tensor using dlpack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "engaged-airplane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['si_address',\n",
       " 'si_bank_acct',\n",
       " 'si_credit_card',\n",
       " 'si_email',\n",
       " 'si_govt_id',\n",
       " 'si_name',\n",
       " 'si_password',\n",
       " 'si_phone_num',\n",
       " 'si_secret_keys',\n",
       " 'si_user']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = list(df.columns)\n",
    "label_names.remove('data')\n",
    "label_names = sorted(label_names)\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7260f82f-7494-43b0-af9e-03dec0dece70",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {t: i for i, t in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recreational-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = from_dlpack(df[label_names].to_dlpack()).type(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-romania",
   "metadata": {},
   "source": [
    "## Transform text using cudf subword tokenizer\n",
    "\n",
    "We will define two tokenizers using the pretrained vocabulary from the originial BERT-base-cased and BERT-base-uncased. We will create hash files from the vocabulary. Then we use one of our functions to transform the `text` column into two padded tensors for our model training-- `input_ids` and `attention_mask` based on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "descending-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one hash file from bert-base-uncased if needed\n",
    "\n",
    "#hash_vocab('resources/bert-base-uncased-vocab.txt', 'resources/bert-base-uncased-hash.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "endangered-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using mini-bert \"google/bert_uncased_L-4_H-256_A-4\" use uncased vocabulary\n",
    "bert_uncased_tokenizer = SubwordTokenizer('resources/bert-base-uncased-hash.txt', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bbecefa-68c8-43e9-9ecf-53d754160012",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_output = bert_uncased_tokenizer(df[\"data\"], max_length=256, max_num_rows=len(df[\"data\"]),\n",
    "                                          padding='max_length', return_tensors='pt', truncation=True,\n",
    "                                          add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-matrix",
   "metadata": {},
   "source": [
    "## Split into train and test sets\n",
    "\n",
    "Create at pytorch dataset, split into testing and training subsets, and load into pytorch dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subsequent-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = TensorDataset(tokenizer_output[\"input_ids\"].type(torch.long),tokenizer_output[\"attention_mask\"], labels)\n",
    "\n",
    "# use pytorch random_split to create training and validation data subsets\n",
    "dataset_size = len(tokenizer_output[\"input_ids\"])\n",
    "train_size = int(dataset_size * .8) # 80/20 split\n",
    "training_dataset, validation_dataset = random_split(dataset, (train_size, (dataset_size-train_size)))\n",
    "\n",
    "# create dataloaders\n",
    "train_dataloader = DataLoader(dataset=training_dataset, shuffle=True, batch_size=32)\n",
    "val_dataloader = DataLoader(dataset=validation_dataset, shuffle=False, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-volleyball",
   "metadata": {},
   "source": [
    "## Load pretrained model from huggingface repo or fine-tune a morpheus pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "municipal-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(label_names)\n",
    "\n",
    "# load the following model for mini-bert from huggingface\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"google/bert_uncased_L-4_H-256_A-4\", num_labels=num_labels)\n",
    "\n",
    "model = torch.load('repo_model/sid-minibert-20211021.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "brave-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model.cuda(); # move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hearing-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of gpus\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "# use DataParallel if you have more than one GPU\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-tennessee",
   "metadata": {},
   "source": [
    "## Fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "educational-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using hyperparameters recommended in orginial BERT paper\n",
    "# the optimizer allows us to apply different hyperpameters for specific parameter groups\n",
    "# apply weight decay to all parameters other than bias, gamma, and beta\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "disciplinary-values",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:24<00:00, 24.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0006268636239110492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# number of training epochs, keep low to avoid overfitting\n",
    "epochs = 1\n",
    "\n",
    "# train loop\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  # tracking variables\n",
    "    tr_loss = 0 #running loss\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # train the data for one epoch\n",
    "    for batch in train_dataloader:\n",
    "        # unpack the inputs from dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # clear out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        # using binary cross-entropy with logits as loss function\n",
    "        # assigns independent probabilities to each label\n",
    "        loss_func = BCEWithLogitsLoss() \n",
    "        loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation \n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu parallel training\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-revelation",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "We evaluate the accuracy on the 20% of data we have in the validation set. We report the `F1 macro accuracy`- correct_predictions divided by total_predictions is calculated for each label and averaged, and the `flat accuracy`- correct_predictions divided by total_predctions of the model for the validation set as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "amended-organic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Macro Validation Accuracy:  99.87012987012986\n",
      "Flat Validation Accuracy:  99.75\n"
     ]
    }
   ],
   "source": [
    "# model to eval mode to evaluate loss on the validation set\n",
    "model.eval()\n",
    "\n",
    "# variables to gather full output\n",
    "logit_preds,true_labels,pred_labels = [],[],[]\n",
    "\n",
    "# predict\n",
    "for batch in val_dataloader:\n",
    "    # unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        # forward pass\n",
    "        output = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        b_logit_pred = output[0]\n",
    "        b_pred_label = torch.sigmoid(b_logit_pred)\n",
    "        b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "        b_pred_label = b_pred_label.detach().cpu().numpy()\n",
    "        b_labels = b_labels.detach().cpu().numpy()\n",
    "    \n",
    "    logit_preds.extend(b_logit_pred)\n",
    "    true_labels.extend(b_labels)\n",
    "    pred_labels.extend(b_pred_label)\n",
    "\n",
    "# calculate accuracy, using 0.50 threshold\n",
    "threshold = 0.50\n",
    "pred_bools = [pl>threshold for pl in pred_labels]\n",
    "true_bools = [tl==1 for tl in true_labels]\n",
    "val_f1_accuracy = f1_score(true_bools,pred_bools,average='macro')*100\n",
    "val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "print('F1 Macro Validation Accuracy: ', val_f1_accuracy)\n",
    "print('Flat Validation Accuracy: ', val_flat_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "elder-ability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si_address\n",
      "[[370   0]\n",
      " [  0  30]]\n",
      "si_bank_acct\n",
      "[[354   0]\n",
      " [  0  46]]\n",
      "si_credit_card\n",
      "[[357   0]\n",
      " [  0  43]]\n",
      "si_email\n",
      "[[362   0]\n",
      " [  0  38]]\n",
      "si_govt_id\n",
      "[[361   0]\n",
      " [  0  39]]\n",
      "si_name\n",
      "[[361   1]\n",
      " [  0  38]]\n",
      "si_password\n",
      "[[357   0]\n",
      " [  0  43]]\n",
      "si_phone_num\n",
      "[[355   0]\n",
      " [  0  45]]\n",
      "si_secret_keys\n",
      "[[365   0]\n",
      " [  0  35]]\n",
      "si_user\n",
      "[[365   0]\n",
      " [  0  35]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for each label\n",
    "\n",
    "for label, cf in zip(label_names, multilabel_confusion_matrix(true_bools, pred_bools)):\n",
    "                     print(label)\n",
    "                     print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-poker",
   "metadata": {},
   "source": [
    "## Save model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "further-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model = model.module\n",
    "\n",
    "# torch.save(model, output_file)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-community",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Using pretrained BERT model (`mini-bert`) from the huggingface repo or the morpheus repo and a custom traning for multi-label classification, we are able to train a sensitive information detector from our PCAP labeled training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
