{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "distributed-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rmm\n",
    "import cudf\n",
    "import cugraph\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "default_palette = [\n",
    "    # https://colorbrewer2.org/#type=diverging&scheme=Spectral&n=11\n",
    "    4288545090, 4292165199,\n",
    "    4294208835, 4294815329,\n",
    "    4294893707, 4294967231,\n",
    "    4293326232, 4289453476,\n",
    "    4284924581, 4281501885,\n",
    "    4284370850\n",
    "]\n",
    "\n",
    "white = 0x02ffffff\n",
    "red = 0x80ff0000\n",
    "yellow = 0x1ef7f763\n",
    "\n",
    "def print_df(name, df):\n",
    "    print(str(name) + \" dtypes:\\n\" + str(df.dtypes))\n",
    "    print(str(name) + \":\\n\" + str(df))\n",
    "\n",
    "\n",
    "def category_to_color(categories, color_palette=None, cat_colors: dict = None):\n",
    "\n",
    "    if color_palette is None:\n",
    "        color_palette = default_palette\n",
    "    color_indices = cudf.Series(categories)\n",
    "    color_palette = cudf.Series(color_palette)\n",
    "\n",
    "    # Check if we need to convert from string to indices\n",
    "    if color_indices.dtype.type != np.uint32:\n",
    "        if (cat_colors is not None):\n",
    "            # Use the category names to convert to codes to ensure the proper indexes\n",
    "            color_indices = cudf.Series(categories,\n",
    "                                        dtype=\"category\").cat.set_categories(list(cat_colors.keys()))\n",
    "\n",
    "            assert not color_indices.isna().any()\n",
    "\n",
    "            # Now convert to uint32 codes\n",
    "            color_indices = color_indices.cat.codes.astype(np.uint32)\n",
    "        else:\n",
    "            # Auto factorize\n",
    "            color_indices = cudf.Series(categories.factorize()[0]).astype(np.uint32)\n",
    "\n",
    "    # Set the color_palette if we have a category dictionary\n",
    "    if (cat_colors is not None):\n",
    "        color_palette = cudf.Series(list(cat_colors.values()))\n",
    "\n",
    "    color_palettes = []\n",
    "    num_color_ids = color_indices.max() + 1\n",
    "    for i in range(ceil(num_color_ids / len(color_palette))):\n",
    "        color_palettes.append(color_palette)\n",
    "    return cudf.Series(\n",
    "        cudf.core.column.build_categorical_column(\n",
    "            ordered=True,\n",
    "            codes=color_indices._column,\n",
    "            categories=cudf.concat(color_palettes)[:num_color_ids],\n",
    "        ).as_numerical_column(dtype=np.uint32))\n",
    "\n",
    "\n",
    "def compute_edge_bundles(edges, id_, src, dst):\n",
    "\n",
    "    def drop_index(df):\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def smoosh(df):\n",
    "        size = sum([df[x].dtype.itemsize for x in df])\n",
    "        data = drop_index(drop_index(df).stack()).data\n",
    "        dtype = cudf.utils.dtypes.min_unsigned_type(0, size*8)\n",
    "        return cudf.core.column.NumericalColumn(data, dtype=dtype)\n",
    "\n",
    "    edges = cudf.DataFrame({\n",
    "        \"eid\": drop_index(edges[id_]),\n",
    "        \"src\": drop_index(edges[src]),\n",
    "        \"dst\": drop_index(edges[dst]),\n",
    "    })\n",
    "    # Create a duplicate table with:\n",
    "    # * all the [src, dst] in the upper half\n",
    "    # * all the [dst, src] pairs as the lower half, but flipped so dst->src, src->dst\n",
    "    bundles = drop_index(cudf.DataFrame({\n",
    "        \"eid\": cudf.concat([edges[\"eid\"], edges[\"eid\"]], ignore_index=True),\n",
    "        # concat [src, dst] into the \"src\" column\n",
    "        \"src\": cudf.concat([edges[\"src\"], edges[\"dst\"]], ignore_index=True),\n",
    "        # concat [dst, src] into the \"dst\" column\n",
    "        \"dst\": cudf.concat([edges[\"dst\"], edges[\"src\"]], ignore_index=True),\n",
    "    }))\n",
    "\n",
    "    # Group the duplicated edgelist by [src, dst] and get the min edge id.\n",
    "    # Since all the [dst, src] pairs have been flipped to [src, dst], each\n",
    "    # edge with the same [src, dst] or [dst, src] vertices will be assigned\n",
    "    # the same bundle id\n",
    "    bundles = bundles \\\n",
    "        .groupby([\"src\", \"dst\"]).agg({\"eid\": \"min\"}) \\\n",
    "        .reset_index().rename(columns={\"eid\": \"bid\"}, copy=False)\n",
    "\n",
    "    # Join the bundle ids into the edgelist\n",
    "    edges = edges.merge(bundles, on=[\"src\", \"dst\"], how=\"inner\")\n",
    "\n",
    "    # Determine each bundle\"s size and relative offset\n",
    "    lengths = edges[\"bid\"].value_counts(sort=False).sort_index()\n",
    "    bundles = lengths.index.to_series().unique()\n",
    "    offsets = lengths.cumsum() - lengths\n",
    "\n",
    "    # Join the bundle segment lengths + offsets into the edgelist\n",
    "    edges = edges.merge(cudf.DataFrame({\n",
    "        \"bid\": drop_index(bundles.astype(np.uint32)),\n",
    "        \"start\": drop_index(offsets.astype(np.uint32)),\n",
    "        \"count\": drop_index(lengths.astype(np.uint32)),\n",
    "    }), on=\"bid\", how=\"left\")\n",
    "\n",
    "    # Determine each edge's index relative to its bundle\n",
    "    edges = drop_index(edges.sort_values(by=\"bid\"))\n",
    "    edges[\"index\"] = edges.index.to_series() - edges[\"start\"]\n",
    "    edges[\"index\"] = edges[\"index\"].astype(np.uint32)\n",
    "\n",
    "    # Re-sort the edgelist by edge id and cleanup\n",
    "    edges = drop_index(edges.sort_values(by=\"eid\"))\n",
    "    edges = edges.rename(columns={\"eid\": \"id\"}, copy=False)\n",
    "    edges = edges[[\"id\", \"src\", \"dst\", \"index\", \"count\"]]\n",
    "\n",
    "    return {\n",
    "        \"edge\": smoosh(edges[[\"src\", \"dst\"]]).astype(np.uint64),\n",
    "        \"bundle\": smoosh(edges[[\"index\", \"count\"]]).astype(np.uint64),\n",
    "    }\n",
    "\n",
    "\n",
    "def from_cudf_edgelist(df, source=\"src\", target=\"dst\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Construct an enhanced graph from a cuDF edgelist that doesn't collapse\n",
    "    duplicate edges and includes columns for node degree and edge bundle.\n",
    "    \"\"\"\n",
    "\n",
    "    def drop_index(df):\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def smoosh(df):\n",
    "        size = sum([df[x].dtype.itemsize for x in df])\n",
    "        data = drop_index(drop_index(df).stack()).data\n",
    "        dtype = cudf.utils.dtypes.min_unsigned_type(0, size*8)\n",
    "        return cudf.core.column.NumericalColumn(data, dtype=dtype)\n",
    "\n",
    "    def make_nodes(df, src, dst):\n",
    "        nodes = drop_index(df[src].append(df[dst], ignore_index=True).unique())\n",
    "        ids = drop_index(cudf.Series(nodes.factorize()[0])).astype(np.uint32)\n",
    "        return drop_index(cudf.DataFrame({\"id\": ids, \"node\": nodes}).sort_values(by=\"id\"))\n",
    "\n",
    "    def make_edges(df, src, dst, nodes):\n",
    "        def join(edges, nodes, col):\n",
    "            edges = edges.set_index(col, drop=True)\n",
    "            nodes = nodes.set_index(\"node\", drop=True)\n",
    "            edges = edges.join(nodes).sort_values(by=\"eid\")\n",
    "            edges = edges.rename(columns={\"id\": col}, copy=False)\n",
    "            return drop_index(edges)\n",
    "        edges = df.reset_index().rename(columns={\"index\": \"eid\"}, copy=False)\n",
    "        edges = join(join(edges.assign(src=df[src], dst=df[dst]), nodes, \"src\"), nodes, \"dst\")\n",
    "        return drop_index(edges.rename(columns={\"eid\": \"id\"}, copy=False))\n",
    "\n",
    "    df = drop_index(df)\n",
    "    graph = cugraph.MultiDiGraph()\n",
    "    nodes = make_nodes(df, source, target)\n",
    "    edges = make_edges(df, source, target, nodes)\n",
    "    graph.edgelist = cugraph.Graph.EdgeList(edges[\"src\"], edges[\"dst\"])\n",
    "    nodes = nodes.set_index(\"id\", drop=False).join(graph.degree().set_index(\"vertex\"))\n",
    "    return graph, drop_index(nodes.sort_index()), edges\n",
    "\n",
    "\n",
    "def annotate_nodes(graph, nodes, edges):\n",
    "    return nodes.assign(\n",
    "        # add node names\n",
    "        name=nodes[\"name\"] if \"name\" in nodes else nodes[\"id\"],\n",
    "        # add node sizes\n",
    "        size=(nodes[\"degree\"].scale() * (100 - 2) + 2).astype(np.uint8),\n",
    "        # add node colors\n",
    "        color=category_to_color(\n",
    "            cugraph.spectralBalancedCutClustering(graph, min(9, graph.number_of_nodes() - 1))\n",
    "                   .sort_values(by=\"vertex\").reset_index(drop=True)[\"cluster\"],\n",
    "            color_palette=[\n",
    "                # Make all nodes white\n",
    "                4294967295\n",
    "#                 # https://colorbrewer2.org/#type=diverging&scheme=Spectral&n=9\n",
    "#                 4292165199, 4294208835,\n",
    "#                 4294815329, 4294893707,\n",
    "#                 4294967231, 4293326232,\n",
    "#                 4289453476, 4284924581,\n",
    "#                 4281501885\n",
    "            ])\n",
    "    )\n",
    "\n",
    "\n",
    "def annotate_edges(graph, nodes, edges):\n",
    "    def drop_index(df):\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def smoosh(df):\n",
    "        size = sum([df[x].dtype.itemsize for x in df])\n",
    "        data = drop_index(drop_index(df).stack()).data\n",
    "        dtype = cudf.utils.dtypes.min_unsigned_type(0, size*8)\n",
    "        return cudf.core.column.NumericalColumn(data, dtype=dtype)\n",
    "\n",
    "    def edge_colors(nodes, edges, col):\n",
    "        edges = edges[[\"id\", col]].set_index(col, drop=True)\n",
    "        nodes = nodes[[\"id\", \"color\"]].set_index(\"id\", drop=True)\n",
    "        return drop_index(edges.join(nodes).sort_values(by=\"id\")[\"color\"])\n",
    "\n",
    "    return edges.assign(\n",
    "        # add edge names\n",
    "        name=edges[\"name\"] if \"name\" in edges else edges[\"id\"],\n",
    "        # add edge colors\n",
    "        color=smoosh(cudf.DataFrame({\n",
    "            \"src\": edge_colors(nodes, edges, \"src\"),\n",
    "            \"dst\": edge_colors(nodes, edges, \"dst\"),\n",
    "        })))\n",
    "\n",
    "\n",
    "def make_capwin_graph(df):\n",
    "\n",
    "    def drop_index(df):\n",
    "        return df.reset_index(drop=True)\n",
    "\n",
    "    def smoosh(df):\n",
    "        size = sum([df[x].dtype.itemsize for x in df])\n",
    "        data = drop_index(drop_index(df).stack()).data\n",
    "        dtype = cudf.utils.dtypes.min_unsigned_type(0, size*8)\n",
    "        return cudf.core.column.NumericalColumn(data, dtype=dtype)\n",
    "\n",
    "    def add_edge_colors(edges, category):\n",
    "        colors = drop_index(\n",
    "            category_to_color(\n",
    "                edges[category],\n",
    "                color_palette=[\n",
    "                    #     FALSE,      TRUE\n",
    "                    # 268435455, 268369920\n",
    "\n",
    "                    #    FALSE,       TRUE\n",
    "                    # 33554431, 4293138972\n",
    "\n",
    "                    #    ADDRESS   AUTH KEYS CREDENTIALS       EMAIL      FALSE\n",
    "                    # 4294967091, 4294410687, 4293138972, 4281827000,  33554431\n",
    "                    0x0fffeda0,  # address 268430752\n",
    "                    0x0ffed976,  # bank_acct 268360054\n",
    "                    0x0ffeb24c,  # email\n",
    "                    0x0ffd8d3c,  # govt_id\n",
    "                    0x0ffc4e2a,  # name\n",
    "                    0x01ffffff,  # none 33554431\n",
    "                    0x0fe31a1c,  # phone_num\n",
    "                    0x0fbd0026,  # secret_keys\n",
    "                    0x0f800026,  # user\n",
    "                ],\n",
    "                cat_colors={\n",
    "                    \"address\": white,\n",
    "                    \"bank_acct\": yellow,\n",
    "                    \"credit_card\": yellow,\n",
    "                    \"email\": yellow,\n",
    "                    \"govt_id\": yellow,\n",
    "                    \"name\": yellow,\n",
    "                    \"none\": white,\n",
    "                    \"phone_num\": yellow,\n",
    "                    \"secret_keys\": red,\n",
    "                    \"user\": yellow,\n",
    "                }).astype(np.uint32))\n",
    "        return edges.assign(color=smoosh(cudf.DataFrame({\n",
    "            \"src\": drop_index(colors), \"dst\": drop_index(colors)\n",
    "        })).astype(np.uint64),\n",
    "                            src_color=colors)\n",
    "    \n",
    "    # Create graph\n",
    "    graph, nodes, edges = from_cudf_edgelist(df, \"src_ip\", \"dest_ip\")\n",
    "    # Add vis components \n",
    "    nodes = nodes.rename(columns={\"node\": \"name\"}, copy=False)\n",
    "    nodes = annotate_nodes(graph, nodes, edges)\n",
    "    # add edge colors\n",
    "    edges = add_edge_colors(edges, \"pii\")\n",
    "    # add edge names\n",
    "    edges[\"name\"] = edges[\"src_ip\"] + \" -> \" + edges[\"dest_ip\"] + \\\n",
    "        (\"\\nPII: \" + edges[\"pii\"]).replace(\"\\nPII: FALSE\", \"\")\n",
    "    return graph, nodes, edges\n",
    "\n",
    "\n",
    "def make_capwin_dataset(start, end, src_path, dst_path):\n",
    "\n",
    "    def arange(size, dtype=\"uint32\"):\n",
    "        return cudf.core.index.RangeIndex(0, size).to_series().astype(dtype)\n",
    "\n",
    "    def relabel_nodes(nodesA, nodesB):\n",
    "        nodesA = nodesA.rename(columns={\"id\": \"lhs_id\"}, copy=False).set_index(\"name\", drop=True)\n",
    "        nodesB = nodesB.rename(columns={\"id\": \"rhs_id\"}, copy=False).set_index(\"name\", drop=False)\n",
    "        nodes = nodesA.join(nodesB, how=\"outer\", sort=True) \\\n",
    "                      .sort_values(by=\"lhs_id\").reset_index(drop=True)\n",
    "        nodes = nodes.reset_index().rename(columns={\"index\": \"id\"}, copy=False)\n",
    "        nodes = nodes.drop(columns=[\"lhs_id\"]).rename(columns={\"rhs_id\": \"remap\"}, copy=False)\n",
    "        return nodes \\\n",
    "            .reset_index(drop=True).sort_values(by=\"id\") \\\n",
    "            .reset_index(drop=True).astype({\"id\": np.uint32})\n",
    "    \n",
    "    \n",
    "    def relabel_edges(edgesA, edgesB, nodes):\n",
    "        def remap(edges, nodes, col):\n",
    "            edges = edges.set_index(col)\n",
    "            nodes = nodes.rename(columns={\"id\": col}, copy=False)\n",
    "            nodes = nodes.set_index(\"remap\")\n",
    "            return edges.join(nodes, sort=True).reset_index(drop=True)\n",
    "        \n",
    "        return remap(remap(edgesB, nodes, \"src\"), nodes, \"dst\") \\\n",
    "            .drop(columns=[\"id\"]) \\\n",
    "            .sort_values(by=[\"src\", \"dst\"]).reset_index(drop=True) \\\n",
    "            .reset_index().rename(columns={\"index\": \"id\"}, copy=False)\n",
    "\n",
    "    \n",
    "    df, nodes, edges = (cudf.DataFrame(), None, None)\n",
    "\n",
    "    for i in range(start, end):\n",
    "        # print(f\"reading {src_path}/{i}.0.csv\")\n",
    "        try:\n",
    "            df2 = cudf.read_csv(\n",
    "                f\"{src_path}/{i}.0.csv\",\n",
    "                header=0,\n",
    "                parse_dates=[1],\n",
    "                usecols=[2, 3, 6],\n",
    "                dtype=[\n",
    "                    \"int32\",          # index\n",
    "                    \"datetime64[ms]\", # timestamp\n",
    "                    \"str\",            # src_ip\n",
    "                    \"str\",            # dest_ip\n",
    "                    \"int32\",          # src_port\n",
    "                    \"int32\",          # dest_port\n",
    "                    \"str\"             # pii\n",
    "                ]\n",
    "            ).reset_index(drop=True)\n",
    "        except Exception:\n",
    "            # print(f\"missing {src_path}/{i}.0.csv\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"read {src_path}/{i}.0.csv\")\n",
    "\n",
    "        if \"si\" in df2:\n",
    "            df2 = df2.rename(columns={\"si\":\"pii\"}, copy=False)\n",
    "\n",
    "        df = cudf.concat([df.reset_index(drop=True), df2], ignore_index=True) \\\n",
    "            .reset_index(drop=True).sort_values(by=[\"src_ip\", \"dest_ip\"]) \\\n",
    "            .reset_index(drop=True)\n",
    "        \n",
    "        results = make_capwin_graph(df[[\"src_ip\", \"dest_ip\", \"pii\"]])\n",
    "\n",
    "        if i == 0:\n",
    "            nodes = results[1][[\"id\", \"name\", \"degree\", \"size\", \"color\"]]\n",
    "            edges = results[2][[\"id\", \"name\", \"src\", \"dst\", \"color\"]]\n",
    "        else:\n",
    "            nodes = relabel_nodes(\n",
    "                nodes[[\"id\", \"name\"]],\n",
    "                results[1][[\"id\", \"name\", \"degree\", \"size\", \"color\"]]\n",
    "            )\n",
    "            edges = relabel_edges(\n",
    "                edges[[\"id\", \"name\", \"src\", \"dst\", \"color\"]],\n",
    "                results[2][[\"id\", \"name\", \"src\", \"dst\", \"color\"]],\n",
    "                nodes[[\"id\", \"remap\"]]\n",
    "            )\n",
    "        \n",
    "        edges = edges.assign(**compute_edge_bundles(edges, \"id\", \"src\", \"dst\"))\n",
    "\n",
    "        nodes_out = nodes[[\"name\", \"id\", \"color\", \"size\"]]\n",
    "        edges_out = edges[[\"name\", \"src\", \"dst\", \"edge\", \"color\", \"bundle\"]]\n",
    "        \n",
    "        print(f\"{dst_path}/{i}.0.nodes.csv\")\n",
    "        \n",
    "        nodes_out.to_csv(f\"{dst_path}/{i}.0.nodes.csv\", index=False)\n",
    "        edges_out.to_csv(f\"{dst_path}/{i}.0.edges.csv\", index=False)\n",
    "\n",
    "    # Print all the PII values at the end so we can map the edge colors\n",
    "    print(df[\"pii\"].unique())\n",
    "    df.groupby(\"pii\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "architectural-tulsa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/0.0.nodes.csv\n",
      "./output/1.0.nodes.csv\n",
      "./output/2.0.nodes.csv\n",
      "./output/3.0.nodes.csv\n",
      "./output/4.0.nodes.csv\n",
      "./output/5.0.nodes.csv\n",
      "./output/6.0.nodes.csv\n",
      "./output/7.0.nodes.csv\n",
      "./output/8.0.nodes.csv\n",
      "./output/9.0.nodes.csv\n",
      "./output/10.0.nodes.csv\n",
      "./output/11.0.nodes.csv\n",
      "./output/12.0.nodes.csv\n",
      "./output/13.0.nodes.csv\n",
      "./output/14.0.nodes.csv\n",
      "./output/15.0.nodes.csv\n",
      "./output/16.0.nodes.csv\n",
      "./output/17.0.nodes.csv\n",
      "./output/18.0.nodes.csv\n",
      "./output/19.0.nodes.csv\n",
      "./output/20.0.nodes.csv\n",
      "./output/21.0.nodes.csv\n",
      "./output/22.0.nodes.csv\n",
      "./output/23.0.nodes.csv\n",
      "./output/24.0.nodes.csv\n",
      "./output/25.0.nodes.csv\n",
      "./output/26.0.nodes.csv\n",
      "./output/27.0.nodes.csv\n",
      "./output/28.0.nodes.csv\n",
      "./output/29.0.nodes.csv\n",
      "./output/30.0.nodes.csv\n",
      "./output/31.0.nodes.csv\n",
      "./output/32.0.nodes.csv\n",
      "./output/33.0.nodes.csv\n",
      "./output/34.0.nodes.csv\n",
      "./output/35.0.nodes.csv\n",
      "./output/36.0.nodes.csv\n",
      "./output/37.0.nodes.csv\n",
      "./output/38.0.nodes.csv\n",
      "./output/39.0.nodes.csv\n",
      "./output/40.0.nodes.csv\n",
      "./output/41.0.nodes.csv\n",
      "./output/42.0.nodes.csv\n",
      "./output/43.0.nodes.csv\n",
      "./output/44.0.nodes.csv\n",
      "./output/45.0.nodes.csv\n",
      "./output/46.0.nodes.csv\n",
      "./output/47.0.nodes.csv\n",
      "./output/48.0.nodes.csv\n",
      "./output/49.0.nodes.csv\n",
      "./output/50.0.nodes.csv\n",
      "./output/51.0.nodes.csv\n",
      "./output/52.0.nodes.csv\n",
      "./output/53.0.nodes.csv\n",
      "./output/54.0.nodes.csv\n",
      "./output/55.0.nodes.csv\n",
      "./output/56.0.nodes.csv\n",
      "./output/57.0.nodes.csv\n",
      "./output/58.0.nodes.csv\n",
      "./output/59.0.nodes.csv\n",
      "./output/60.0.nodes.csv\n",
      "./output/61.0.nodes.csv\n",
      "./output/62.0.nodes.csv\n",
      "./output/63.0.nodes.csv\n",
      "./output/64.0.nodes.csv\n",
      "./output/65.0.nodes.csv\n",
      "./output/66.0.nodes.csv\n",
      "./output/67.0.nodes.csv\n",
      "./output/68.0.nodes.csv\n",
      "./output/69.0.nodes.csv\n",
      "./output/70.0.nodes.csv\n",
      "./output/71.0.nodes.csv\n",
      "./output/72.0.nodes.csv\n",
      "./output/73.0.nodes.csv\n",
      "./output/74.0.nodes.csv\n",
      "./output/75.0.nodes.csv\n",
      "./output/76.0.nodes.csv\n",
      "./output/77.0.nodes.csv\n",
      "./output/78.0.nodes.csv\n",
      "./output/79.0.nodes.csv\n",
      "./output/80.0.nodes.csv\n",
      "./output/81.0.nodes.csv\n",
      "./output/82.0.nodes.csv\n",
      "./output/83.0.nodes.csv\n",
      "./output/84.0.nodes.csv\n",
      "./output/85.0.nodes.csv\n",
      "./output/86.0.nodes.csv\n",
      "./output/87.0.nodes.csv\n",
      "./output/88.0.nodes.csv\n",
      "./output/89.0.nodes.csv\n",
      "./output/90.0.nodes.csv\n",
      "./output/91.0.nodes.csv\n",
      "./output/92.0.nodes.csv\n",
      "./output/93.0.nodes.csv\n",
      "./output/94.0.nodes.csv\n",
      "./output/95.0.nodes.csv\n",
      "./output/96.0.nodes.csv\n",
      "./output/97.0.nodes.csv\n",
      "./output/98.0.nodes.csv\n",
      "./output/99.0.nodes.csv\n",
      "./output/100.0.nodes.csv\n",
      "./output/101.0.nodes.csv\n",
      "./output/102.0.nodes.csv\n",
      "./output/103.0.nodes.csv\n",
      "./output/104.0.nodes.csv\n",
      "./output/105.0.nodes.csv\n",
      "./output/106.0.nodes.csv\n",
      "./output/107.0.nodes.csv\n",
      "./output/108.0.nodes.csv\n",
      "./output/109.0.nodes.csv\n",
      "./output/110.0.nodes.csv\n",
      "./output/111.0.nodes.csv\n",
      "./output/112.0.nodes.csv\n",
      "./output/113.0.nodes.csv\n",
      "./output/114.0.nodes.csv\n",
      "./output/115.0.nodes.csv\n",
      "./output/116.0.nodes.csv\n",
      "./output/117.0.nodes.csv\n",
      "./output/118.0.nodes.csv\n",
      "./output/119.0.nodes.csv\n",
      "./output/120.0.nodes.csv\n",
      "./output/121.0.nodes.csv\n",
      "./output/122.0.nodes.csv\n",
      "./output/123.0.nodes.csv\n",
      "./output/124.0.nodes.csv\n",
      "./output/125.0.nodes.csv\n",
      "./output/126.0.nodes.csv\n",
      "./output/127.0.nodes.csv\n",
      "./output/128.0.nodes.csv\n",
      "./output/129.0.nodes.csv\n",
      "./output/130.0.nodes.csv\n",
      "./output/131.0.nodes.csv\n",
      "./output/132.0.nodes.csv\n",
      "./output/133.0.nodes.csv\n",
      "./output/134.0.nodes.csv\n",
      "./output/135.0.nodes.csv\n",
      "./output/136.0.nodes.csv\n",
      "./output/137.0.nodes.csv\n",
      "./output/138.0.nodes.csv\n",
      "./output/139.0.nodes.csv\n",
      "./output/140.0.nodes.csv\n",
      "./output/141.0.nodes.csv\n",
      "./output/142.0.nodes.csv\n",
      "./output/143.0.nodes.csv\n",
      "./output/144.0.nodes.csv\n",
      "./output/145.0.nodes.csv\n",
      "./output/146.0.nodes.csv\n",
      "./output/147.0.nodes.csv\n",
      "./output/148.0.nodes.csv\n",
      "./output/149.0.nodes.csv\n",
      "./output/150.0.nodes.csv\n",
      "./output/151.0.nodes.csv\n",
      "./output/152.0.nodes.csv\n",
      "./output/153.0.nodes.csv\n",
      "./output/154.0.nodes.csv\n",
      "./output/155.0.nodes.csv\n",
      "./output/156.0.nodes.csv\n",
      "./output/157.0.nodes.csv\n",
      "./output/158.0.nodes.csv\n",
      "./output/159.0.nodes.csv\n",
      "./output/160.0.nodes.csv\n",
      "./output/161.0.nodes.csv\n",
      "./output/162.0.nodes.csv\n",
      "./output/163.0.nodes.csv\n",
      "./output/164.0.nodes.csv\n",
      "./output/165.0.nodes.csv\n",
      "./output/166.0.nodes.csv\n",
      "./output/167.0.nodes.csv\n",
      "./output/168.0.nodes.csv\n",
      "./output/169.0.nodes.csv\n",
      "./output/170.0.nodes.csv\n",
      "./output/171.0.nodes.csv\n",
      "./output/172.0.nodes.csv\n",
      "./output/173.0.nodes.csv\n",
      "./output/174.0.nodes.csv\n",
      "./output/175.0.nodes.csv\n",
      "./output/176.0.nodes.csv\n",
      "./output/177.0.nodes.csv\n",
      "./output/178.0.nodes.csv\n",
      "./output/179.0.nodes.csv\n",
      "./output/180.0.nodes.csv\n",
      "./output/181.0.nodes.csv\n",
      "./output/182.0.nodes.csv\n",
      "./output/183.0.nodes.csv\n",
      "./output/184.0.nodes.csv\n",
      "./output/185.0.nodes.csv\n",
      "./output/186.0.nodes.csv\n",
      "./output/187.0.nodes.csv\n",
      "./output/188.0.nodes.csv\n",
      "./output/189.0.nodes.csv\n",
      "./output/190.0.nodes.csv\n",
      "./output/191.0.nodes.csv\n",
      "./output/192.0.nodes.csv\n",
      "./output/193.0.nodes.csv\n",
      "./output/194.0.nodes.csv\n",
      "./output/195.0.nodes.csv\n",
      "./output/196.0.nodes.csv\n",
      "./output/197.0.nodes.csv\n",
      "./output/198.0.nodes.csv\n",
      "./output/199.0.nodes.csv\n",
      "0        address\n",
      "1      bank_acct\n",
      "2          email\n",
      "3        govt_id\n",
      "4           name\n",
      "5           none\n",
      "6      phone_num\n",
      "7    secret_keys\n",
      "8           user\n",
      "Name: pii, dtype: object\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(\"./output\")\n",
    "\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "make_capwin_dataset(0,\n",
    "                    200,\n",
    "                    \"../viz_frames\",\n",
    "                    \"./output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "loving-republican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4294967231,\n",
       " 4288545090,\n",
       " 4288545090,\n",
       " 4294967231,\n",
       " 4288545090,\n",
       " 4294967231,\n",
       " 4288545090,\n",
       " 4294967231,\n",
       " 4288545090,\n",
       " 4288545090,\n",
       " 4294967231]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[int('%02x%02x%02x%02x' % x, 16) for x in default_palette_hex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "suburban-magnet",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-bc928d0bf57f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.groupby(\"pii\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-supplement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
