{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2941e94f-db20-44a5-ab87-2cab499825f7",
   "metadata": {},
   "source": [
    "# Digital Finger Printing (DFP) with Morpheus - DUO Training\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will be building and running a DFP pipeline that performs training on Duo authentication logs. The goal is to train an autoencoder PyTorch model to recogize the patterns of users in the sample data. The model will then be used by a second Morpheus pipeline to generate anomaly scores for each individual log. These anomaly scores can be used by security teams to detect abnormal behavior when it happens so the proper action can be taken.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For more information on DFP, the Morpheus pipeline, and setup steps to run this notebook, please see the coresponding DFP training materials.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1cb50-74f2-445d-b865-8c22c3b3798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the morpheus directory is in the python path. This may not need to be run depending on the environment setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"./morpheus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ce011-3ca3-4f96-a72d-de28fad32003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import typing\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import click\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from dfp.stages.dfp_inference_stage import DFPInferenceStage\n",
    "from dfp.stages.dfp_mlflow_model_writer import DFPMLFlowModelWriterStage\n",
    "from dfp.stages.dfp_postprocessing_stage import DFPPostprocessingStage\n",
    "from dfp.stages.dfp_preprocessing_stage import DFPPreprocessingStage\n",
    "from dfp.stages.dfp_rolling_window_stage import DFPRollingWindowStage\n",
    "from dfp.stages.dfp_s3_batcher_stage import DFPS3BatcherStage\n",
    "from dfp.stages.dfp_s3_to_df import DFPS3ToDataFrameStage\n",
    "from dfp.stages.dfp_split_users_stage import DFPSplitUsersStage\n",
    "from dfp.stages.dfp_training import DFPTraining\n",
    "from dfp.stages.multi_file_source import MultiFileSource\n",
    "from dfp.stages.s3_object_source_stage import S3BucketSourceStage\n",
    "from dfp.stages.s3_object_source_stage import s3_filter_duo\n",
    "from dfp.stages.s3_object_source_stage import s3_object_generator\n",
    "from dfp.stages.write_to_s3_stage import WriteToS3Stage\n",
    "from dfp.utils.column_info import BoolColumn\n",
    "from dfp.utils.column_info import CustomColumn\n",
    "from dfp.utils.column_info import DataFrameInputSchema\n",
    "from dfp.utils.column_info import RenameColumn\n",
    "\n",
    "import cudf\n",
    "\n",
    "from morpheus._lib.file_types import FileTypes\n",
    "from morpheus.config import Config\n",
    "from morpheus.config import ConfigAutoEncoder\n",
    "from morpheus.config import CppConfig\n",
    "from morpheus.messages.message_meta import UserMessageMeta\n",
    "from morpheus.pipeline import LinearPipeline\n",
    "from morpheus.stages.output.write_to_file_stage import WriteToFileStage\n",
    "from morpheus.utils.logger import configure_logging\n",
    "\n",
    "# Left align all tables\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block}'\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38c1b7-ce84-43e0-ac53-280562dc1642",
   "metadata": {},
   "source": [
    "## High Level Configuration\n",
    "\n",
    "The following options significantly alter the functionality of the pipeline. These options are separated from the individual stage options since they may effect more than one stage. Additionally, the matching python script to this notebook, `dfp_pipeline_duo.py`, configures these options via command line arguments.\n",
    "\n",
    "### Options\n",
    "\n",
    "| Name | Type | Description |\n",
    "| --- | --- | :-- |\n",
    "| `train_users` | One of `[\"all\", \"generic\", \"individual\"]` | This indicates which users to train for this pipeline:<ul><li>`\"generic\"`: Combine all users into a single model with the username 'generic_user'. Skips individual users.</li><li>`\"individual\"`: Trains a separate model for each individual user. Skips 'generic_user'.</li><li>`\"all\"`: Combination of `\"generic\"` and `\"individual\"`. Both the 'generic_user' and individual users are trained in the same pipeline.</li></ul>|\n",
    "| `train_users` | List of strings | Any user in this list will be dropped from the pipeline. Useful for debugging to remove automated accounts with many logs. |\n",
    "| `cache_dir` | string | The location to store cached files. To aid with development and reduce bandwidth, the Morpheus pipeline will cache data from several stages of the pipeline. This option configures the location for those caches. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee00703-75c5-46fc-890c-86733da906c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global options\n",
    "train_users = \"all\"\n",
    "\n",
    "# Enter any users to skip here\n",
    "skip_users: typing.List[str] = []\n",
    "\n",
    "# Location where cache objects will be saved\n",
    "cache_dir = \"./.cache/dfp\"\n",
    "\n",
    "# === Derived Options ===\n",
    "# To include the generic, we must be training all or generic\n",
    "include_generic = train_users == \"all\" or train_users == \"generic\"\n",
    "\n",
    "# To include individual, we must be either training or inferring\n",
    "include_individual = train_users != \"generic\"\n",
    "\n",
    "# None indicates we arent training anything\n",
    "is_training = train_users != \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc24c9-c85e-4977-a348-692c8f0aceaa",
   "metadata": {},
   "source": [
    "### Global Config Object\n",
    "Before creating the pipeline, we need to setup logging and set the parameters for the Morpheus config object. This config object is responsible for the following:\n",
    " - Indicating whether to use C++ or Python stages\n",
    "    - C++ stages are not supported for the DFP pipeline. This should always be `False`\n",
    " - Setting the number of threads to use in the pipeline. Defaults to the thread count of the OS.\n",
    " - Sets the feature column names that will be used in model training\n",
    "    - This option allows extra columns to be used in the pipeline that will not be part of the training algorithm.\n",
    "    - The final features that the model will be trained on will be an intersection of this list with the log columns.\n",
    " - The column name that indicates the user's unique identifier\n",
    "    - It is required for DFP to have a user ID column\n",
    " - The column name that indicates the timestamp for the log\n",
    "    - It is required for DFP to know when each log occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abd537-9162-49dc-8e83-d9465592f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the Morpheus logger\n",
    "configure_logging(log_level=logging.DEBUG)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "CppConfig.set_should_use_cpp(False)\n",
    "\n",
    "config.num_threads = os.cpu_count()\n",
    "\n",
    "config.ae = ConfigAutoEncoder()\n",
    "\n",
    "config.ae.feature_columns = [\n",
    "    'accessdevicebrowser', 'accessdeviceos', 'device', 'result', 'reason', 'logcount', \"locincrement\"\n",
    "]\n",
    "config.ae.userid_column_name = \"username\"\n",
    "config.ae.timestamp_column_name = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc192b5-d4a3-43f5-97c4-38515956206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_date_extractor_duo(s3_object):\n",
    "    key_object = s3_object.key\n",
    "\n",
    "    # Extract the timestamp from the file name\n",
    "    ts_object = key_object.split('_')[2].split('.json')[0].replace('T', ' ').replace('Z', '')\n",
    "    ts_object = datetime.strptime(ts_object, '%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    return ts_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a4d53-32b6-4ab8-a5d7-c0104b31c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column names to ensure all data is uniform\n",
    "column_info = [\n",
    "    RenameColumn(name=\"accessdevicebrowser\", dtype=str, input_name=\"access_device.browser\"),\n",
    "    RenameColumn(name=\"accessdeviceos\", dtype=str, input_name=\"access_device.os\"),\n",
    "    RenameColumn(name=\"locationcity\", dtype=str, input_name=\"auth_device.location.city\"),\n",
    "    RenameColumn(name=\"device\", dtype=str, input_name=\"auth_device.name\"),\n",
    "    BoolColumn(name=\"result\",\n",
    "               dtype=bool,\n",
    "               input_name=\"result\",\n",
    "               true_values=[\"success\", \"SUCCESS\"],\n",
    "               false_values=[\"denied\", \"DENIED\", \"FRAUD\"]),\n",
    "    RenameColumn(name=\"reason\", dtype=str, input_name=\"reason\"),\n",
    "    RenameColumn(name=\"username\", dtype=str, input_name=\"user.name\"),\n",
    "    RenameColumn(name=config.ae.timestamp_column_name, dtype=datetime, input_name=config.ae.timestamp_column_name),\n",
    "]\n",
    "\n",
    "input_schema = DataFrameInputSchema(json_columns=[\"access_device\", \"application\", \"auth_device\", \"user\"],\n",
    "                                    column_info=column_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0cb0a-e65a-444a-a06c-a4525d543790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the final set of columns necessary just before pre-processing\n",
    "def column_logcount(df: cudf.DataFrame):\n",
    "    per_day = df[config.ae.timestamp_column_name].dt.to_period(\"D\")\n",
    "\n",
    "    # Create the per-user, per-day log count\n",
    "    return df.groupby([config.ae.userid_column_name, per_day]).cumcount()\n",
    "\n",
    "def column_locincrement(df: cudf.DataFrame):\n",
    "    per_day = df[config.ae.timestamp_column_name].dt.to_period(\"D\")\n",
    "\n",
    "    # Simple but probably incorrect calculation\n",
    "    return df.groupby([config.ae.userid_column_name, per_day, \"locationcity\"]).ngroup() + 1\n",
    "\n",
    "model_column_info = [\n",
    "    # Input columns\n",
    "    RenameColumn(name=\"accessdevicebrowser\", dtype=str, input_name=\"accessdevicebrowser\"),\n",
    "    RenameColumn(name=\"accessdeviceos\", dtype=str, input_name=\"accessdeviceos\"),\n",
    "    RenameColumn(name=\"device\", dtype=str, input_name=\"device\"),\n",
    "    RenameColumn(name=\"result\", dtype=bool, input_name=\"result\"),\n",
    "    RenameColumn(name=\"reason\", dtype=str, input_name=\"reason\"),\n",
    "    # Derived columns\n",
    "    CustomColumn(name=\"logcount\", dtype=int, process_column_fn=column_logcount),\n",
    "    CustomColumn(name=\"locincrement\", dtype=int, process_column_fn=column_locincrement),\n",
    "    # Extra columns\n",
    "    RenameColumn(name=\"username\", dtype=str, input_name=\"username\"),\n",
    "    RenameColumn(name=config.ae.timestamp_column_name, dtype=datetime, input_name=config.ae.timestamp_column_name),\n",
    "]\n",
    "\n",
    "model_schema = DataFrameInputSchema(column_info=model_column_info, preserve_columns=[\"_batch_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc59de-dea8-4e5f-98e3-98eba1e4621d",
   "metadata": {},
   "source": [
    "## Pipeline Construction\n",
    "From this point on we begin constructing the stages that will make up the pipeline. To make testing easier, constructing the pipeline object, adding the stages, and running the pipeline, is provided as a single cell. The below cell can be rerun multiple times as needed for debugging.\n",
    "\n",
    "### Source Stage (`MultiFileSource`)\n",
    "\n",
    "This pipeline read input logs from one or more input files. This source stage will read all specified log files, combine them into a single `DataFrame`, and pass it into the pipeline. Once all of the logs have been read, the source completes. \n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `input_schema` | `DataFrameInputSchema` | | After the raw `DataFrame` is read from each file, this schema will be applied to ensure a consisten output from the source. |\n",
    "| `filenames` | List of strings | | Any files to read into the pipeline. All files will be combined into a single `DataFrame` |\n",
    "| `parser_kwargs` | `dict` | `{}` | This dictionary will be passed to the `DataFrame` parser class. Allows for customization of log parsing. |\n",
    "\n",
    "\n",
    "### Split Users Stage (`DFPSplitUsersStage`)\n",
    "\n",
    "Once the input logs have been read into a `DataFrame`, this stage is responsible for breaking that single `DataFrame` with many users into multiple `DataFrame`s for each user. This is also where the pipeline chooses whether to train individual users or the generic user (or both).\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `include_generic` | `bool` | | Whether or not to combine all user logs into a single `DataFrame` with the username 'generic_user' |\n",
    "| `include_individual` | `bool` | | Whether or not to output individual `DataFrame` objects for each user |\n",
    "| `skip_users` | List of `str` | `[]` | Any users to remove from the `DataFrame`. Useful for debugging to remove automated accounts with many logs. |\n",
    "\n",
    "### Rolling Window Stage (`DFPRollingWindowStage`)\n",
    "\n",
    "The Rolling Window Stage performs several key pieces of functionality for DFP.\n",
    "1. This stage keeps a moving window of logs on a per user basis\n",
    "   1. These logs are saved to disk to reduce memory requirements between logs from the same user\n",
    "1. It only emits logs when the window history requirements are met\n",
    "   1. Until all of the window history requirements are met, no messages will be sent to the rest of the pipeline.\n",
    "   1. See the below options for configuring the window history requirements\n",
    "1. It repeats the necessary logs to properly calculate log dependent features.\n",
    "   1. To support all column feature types, incoming log messages can be combined with existing history and sent to downstream stages.\n",
    "   1. For example, to calculate a feature that increments a counter for the number of logs a particular user has generated in a single day, we must have the user's log history for the past 24 hours. To support this, this stage will combine new logs with existing history into a single `DataFrame`.\n",
    "   1. It is the responsibility of downstream stages to distinguish between new logs and existing history.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `min_history` | `int` | `300` | The minimum number of logs a user must have before emitting any messages. Logs below this threshold will be saved to disk. |\n",
    "| `min_increment` | `int` or `str` | `300` | Once the min history requirement is met, this stage must receive `min_increment` *new* logs before emmitting another message. Logs received before this threshold is met will be saved to disk. Can be specified as an integer count or a string duration. |\n",
    "| `max_history` | `int` or `str` | `\"60d\"` | Once `min_history` and `min_increment` requirements have been met, this puts an upper bound on the maximum number of messages to forward into the pipeline and also the maximum amount of messages to retain in the history. Can be specified as an integer count or a string duration. |\n",
    "| `cache_dir` | `str` | `./.cache/dfp` | The location to write log history to disk. |\n",
    "\n",
    "### Preprocessing Stage (`DFPPreprocessingStage`)\n",
    "\n",
    "This stage performs the final, row dependent, feature calculations as specified by the input schema object. Once calculated, this stage can forward on all received logs, or optionally can only forward on new logs, removing any history information.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `input_schema` | `DataFrameInputSchema` | | The final, row dependent, schema to apply to the incoming columns |\n",
    "| `only_last_batch` | `bool` | | Whether or not to foward on all received logs, or just new logs. |\n",
    "\n",
    "### Training Stage (`DFPTraining`)\n",
    "\n",
    "This stage is responsible for performing the actual training calculations. Training will be performed on all received data. Resulting message will contain the input data paired with the trained model.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `model_kwargs` | `dict` | `{}` | The options to use when creating a new model instance. See `DFPAutoEncoder` for information on the available options. |\n",
    "\n",
    "### MLFlow Model Writer Stage (`DFPMLFlowModelWriterStage`)\n",
    "\n",
    "This stage is the last step in training. It will upload the trained model from the previous stage to MLFlow. The tracking URI for which MLFlow instance to use is configured using the static method `mlflow.set_tracking_uri()`.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `model_prefix` | `str` | `\"\"` | A prefix to append to the user ID when constructing the MLFlow model name. The final model name will be in the form of `{model_previx}{user_id}` |\n",
    "| `experiment_name` | `str` |  | All models are created inside of an experiment to allow metrics to be saved with each model. This option specifies the experiment name. The final experiment name for each model will be in the form of `{experiment_name}/{model_previx}{user_id}` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825390ad-ce64-4949-b324-33039ffdf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear pipeline object\n",
    "pipeline = LinearPipeline(config)\n",
    "\n",
    "# Source stage uses \n",
    "pipeline.set_source(\n",
    "    MultiFileSource(config,\n",
    "                    input_schema=input_schema,\n",
    "                    filenames=[\"/work/examples/data/dfp/duo/duotest_pt1.json\", \"/work/examples/data/dfp/duo/duotest_pt2.json\", \"/work/examples/data/dfp/duo/duotest_pt3.json\", \"/work/examples/data/dfp/duo/duotest_pt4.json\"],\n",
    "                    parser_kwargs={\n",
    "                        \"lines\": False, \"orient\": \"records\"\n",
    "                    }))\n",
    "\n",
    "# This will split users or just use one single user\n",
    "pipeline.add_stage(\n",
    "    DFPSplitUsersStage(config,\n",
    "                       include_generic=include_generic,\n",
    "                       include_individual=include_individual,\n",
    "                       skip_users=skip_users))\n",
    "\n",
    "# Next, have a stage that will create rolling windows\n",
    "pipeline.add_stage(\n",
    "    DFPRollingWindowStage(\n",
    "        config,\n",
    "        min_history=300 if is_training else 1,\n",
    "        min_increment=300 if is_training else 1,\n",
    "        # For inference, we only ever want 1 day max\n",
    "        max_history=\"5d\" if is_training else \"1d\",\n",
    "        cache_dir=cache_dir))\n",
    "\n",
    "# Output is UserMessageMeta -- Cached frame set\n",
    "pipeline.add_stage(DFPPreprocessingStage(config, input_schema=model_schema, only_last_batch=not is_training))\n",
    "\n",
    "# Finally, perform training which will output a model\n",
    "pipeline.add_stage(DFPTraining(config))\n",
    "\n",
    "# Write that model to MLFlow\n",
    "pipeline.add_stage(DFPMLFlowModelWriterStage(config, model_prefix=\"AE-duo-\", experiment_name=\"DFP-duo-training\"))\n",
    "\n",
    "# Run the pipeline\n",
    "await pipeline._do_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:morpheus] *",
   "language": "python",
   "name": "conda-env-morpheus-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e26783b24f020aa0bcaa00e6ba122db5d0e3da2d892d80be664969895e06a7e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
