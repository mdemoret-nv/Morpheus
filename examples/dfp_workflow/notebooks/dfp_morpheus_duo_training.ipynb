{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9608b50b-b049-424b-b2c0-59973f191533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_UnixSelectorEventLoop running=True closed=False debug=False>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "asyncio.get_running_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49f13ea-63d6-4128-b606-deeba2daef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c1cb50-74f2-445d-b865-8c22c3b3798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/work/examples/morpheus', '/work/examples/dfp_workflow', '/opt/conda/envs/morpheus/lib/python38.zip', '/opt/conda/envs/morpheus/lib/python3.8', '/opt/conda/envs/morpheus/lib/python3.8/lib-dynload', '', '/opt/conda/envs/morpheus/lib/python3.8/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../morpheus\"))\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102ce011-3ca3-4f96-a72d-de28fad32003",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dfp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/mdemoret/Repos/morpheus/morpheus-dev2/examples/dfp_workflow/notebooks/dfp_morpheus_duo_training.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656d6f7265744d2d5a3834302d32227d/home/mdemoret/Repos/morpheus/morpheus-dev2/examples/dfp_workflow/notebooks/dfp_morpheus_duo_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmlflow\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656d6f7265744d2d5a3834302d32227d/home/mdemoret/Repos/morpheus/morpheus-dev2/examples/dfp_workflow/notebooks/dfp_morpheus_duo_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656d6f7265744d2d5a3834302d32227d/home/mdemoret/Repos/morpheus/morpheus-dev2/examples/dfp_workflow/notebooks/dfp_morpheus_duo_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdfp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdfp_inference_stage\u001b[39;00m \u001b[39mimport\u001b[39;00m DFPInferenceStage\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656d6f7265744d2d5a3834302d32227d/home/mdemoret/Repos/morpheus/morpheus-dev2/examples/dfp_workflow/notebooks/dfp_morpheus_duo_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdfp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdfp_mlflow_model_writer\u001b[39;00m \u001b[39mimport\u001b[39;00m DFPMLFlowModelWriterStage\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656d6f7265744d2d5a3834302d32227d/home/mdemoret/Repos/morpheus/morpheus-dev2/examples/dfp_workflow/notebooks/dfp_morpheus_duo_training.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdfp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstages\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdfp_postprocessing_stage\u001b[39;00m \u001b[39mimport\u001b[39;00m DFPPostprocessingStage\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dfp'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import typing\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import click\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from dfp.stages.dfp_inference_stage import DFPInferenceStage\n",
    "from dfp.stages.dfp_mlflow_model_writer import DFPMLFlowModelWriterStage\n",
    "from dfp.stages.dfp_postprocessing_stage import DFPPostprocessingStage\n",
    "from dfp.stages.dfp_preprocessing_stage import DFPPreprocessingStage\n",
    "from dfp.stages.dfp_rolling_window_stage import DFPRollingWindowStage\n",
    "from dfp.stages.dfp_s3_batcher_stage import DFPS3BatcherStage\n",
    "from dfp.stages.dfp_s3_to_df import DFPS3ToDataFrameStage\n",
    "from dfp.stages.dfp_split_users_stage import DFPSplitUsersStage\n",
    "from dfp.stages.dfp_training import DFPTraining\n",
    "from dfp.stages.multi_file_source import MultiFileSource\n",
    "from dfp.stages.s3_object_source_stage import S3BucketSourceStage\n",
    "from dfp.stages.s3_object_source_stage import s3_filter_duo\n",
    "from dfp.stages.s3_object_source_stage import s3_object_generator\n",
    "from dfp.stages.write_to_s3_stage import WriteToS3Stage\n",
    "from dfp.utils.column_info import BoolColumn\n",
    "from dfp.utils.column_info import CustomColumn\n",
    "from dfp.utils.column_info import DataFrameInputSchema\n",
    "from dfp.utils.column_info import RenameColumn\n",
    "\n",
    "import cudf\n",
    "\n",
    "from morpheus._lib.file_types import FileTypes\n",
    "from morpheus.config import Config\n",
    "from morpheus.config import ConfigAutoEncoder\n",
    "from morpheus.config import CppConfig\n",
    "from morpheus.messages.message_meta import UserMessageMeta\n",
    "from morpheus.pipeline import LinearPipeline\n",
    "from morpheus.stages.output.write_to_file_stage import WriteToFileStage\n",
    "from morpheus.utils.logger import configure_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee00703-75c5-46fc-890c-86733da906c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global options\n",
    "train_users = \"all\"\n",
    "\n",
    "# To include the generic, we must be training all or generic\n",
    "include_generic = train_users == \"all\" or train_users == \"generic\"\n",
    "\n",
    "# To include individual, we must be either training or inferring\n",
    "include_individual = train_users != \"generic\"\n",
    "\n",
    "# None indicates we arent training anything\n",
    "is_training = train_users != \"none\"\n",
    "\n",
    "# Enter any users to skip here\n",
    "skip_users: typing.List[str] = None\n",
    "\n",
    "# Location where cache objects will be saved\n",
    "cache_dir = \"./.cache/dfp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abd537-9162-49dc-8e83-d9465592f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "CppConfig.set_should_use_cpp(False)\n",
    "\n",
    "config.num_threads = os.cpu_count()\n",
    "\n",
    "config.ae = ConfigAutoEncoder()\n",
    "\n",
    "config.ae.feature_columns = [\n",
    "    'accessdevicebrowser', 'accessdeviceos', 'device', 'result', 'reason', 'logcount', \"locincrement\"\n",
    "]\n",
    "config.ae.userid_column_name = \"username\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc192b5-d4a3-43f5-97c4-38515956206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_date_extractor_duo(s3_object):\n",
    "    key_object = s3_object.key\n",
    "\n",
    "    # Extract the timestamp from the file name\n",
    "    ts_object = key_object.split('_')[2].split('.json')[0].replace('T', ' ').replace('Z', '')\n",
    "    ts_object = datetime.strptime(ts_object, '%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "    return ts_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a4d53-32b6-4ab8-a5d7-c0104b31c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column names to ensure all data is uniform\n",
    "column_info = [\n",
    "    RenameColumn(name=\"accessdevicebrowser\", dtype=str, input_name=\"access_device.browser\"),\n",
    "    RenameColumn(name=\"accessdeviceos\", dtype=str, input_name=\"access_device.os\"),\n",
    "    RenameColumn(name=\"locationcity\", dtype=str, input_name=\"auth_device.location.city\"),\n",
    "    RenameColumn(name=\"device\", dtype=str, input_name=\"auth_device.name\"),\n",
    "    BoolColumn(name=\"result\",\n",
    "               dtype=bool,\n",
    "               input_name=\"result\",\n",
    "               true_values=[\"success\", \"SUCCESS\"],\n",
    "               false_values=[\"denied\", \"DENIED\", \"FRAUD\"]),\n",
    "    RenameColumn(name=\"reason\", dtype=str, input_name=\"reason\"),\n",
    "    RenameColumn(name=\"username\", dtype=str, input_name=\"user.name\"),\n",
    "    RenameColumn(name=config.ae.timestamp_column_name, dtype=datetime, input_name=config.ae.timestamp_column_name),\n",
    "]\n",
    "\n",
    "input_schema = DataFrameInputSchema(json_columns=[\"access_device\", \"application\", \"auth_device\", \"user\"],\n",
    "                                    column_info=column_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0cb0a-e65a-444a-a06c-a4525d543790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the final set of columns necessary just before pre-processing\n",
    "def column_logcount(df: cudf.DataFrame):\n",
    "    per_day = df[config.ae.timestamp_column_name].dt.to_period(\"D\")\n",
    "\n",
    "    # Create the per-user, per-day log count\n",
    "    return df.groupby([config.ae.userid_column_name, per_day]).cumcount()\n",
    "\n",
    "def column_locincrement(df: cudf.DataFrame):\n",
    "    per_day = df[config.ae.timestamp_column_name].dt.to_period(\"D\")\n",
    "\n",
    "    # Simple but probably incorrect calculation\n",
    "    return df.groupby([config.ae.userid_column_name, per_day, \"locationcity\"]).ngroup() + 1\n",
    "\n",
    "model_column_info = [\n",
    "    # Input columns\n",
    "    RenameColumn(name=\"accessdevicebrowser\", dtype=str, input_name=\"accessdevicebrowser\"),\n",
    "    RenameColumn(name=\"accessdeviceos\", dtype=str, input_name=\"accessdeviceos\"),\n",
    "    RenameColumn(name=\"device\", dtype=str, input_name=\"device\"),\n",
    "    RenameColumn(name=\"result\", dtype=bool, input_name=\"result\"),\n",
    "    RenameColumn(name=\"reason\", dtype=str, input_name=\"reason\"),\n",
    "    # Derived columns\n",
    "    CustomColumn(name=\"logcount\", dtype=int, process_column_fn=column_logcount),\n",
    "    CustomColumn(name=\"locincrement\", dtype=int, process_column_fn=column_locincrement),\n",
    "    # Extra columns\n",
    "    RenameColumn(name=\"username\", dtype=str, input_name=\"username\"),\n",
    "    RenameColumn(name=config.ae.timestamp_column_name, dtype=datetime, input_name=config.ae.timestamp_column_name),\n",
    "]\n",
    "\n",
    "model_schema = DataFrameInputSchema(column_info=model_column_info, preserve_columns=[\"_batch_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825390ad-ce64-4949-b324-33039ffdf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfp.stages.dfp_inference_stage import DFPInferenceStage\n",
    "from dfp.stages.dfp_mlflow_model_writer import DFPMLFlowModelWriterStage\n",
    "from dfp.stages.dfp_postprocessing_stage import DFPPostprocessingStage\n",
    "from dfp.stages.dfp_preprocessing_stage import DFPPreprocessingStage\n",
    "from dfp.stages.dfp_rolling_window_stage import DFPRollingWindowStage\n",
    "from dfp.stages.dfp_s3_batcher_stage import DFPS3BatcherStage\n",
    "from dfp.stages.dfp_s3_to_df import DFPS3ToDataFrameStage\n",
    "from dfp.stages.dfp_split_users_stage import DFPSplitUsersStage\n",
    "from dfp.stages.dfp_training import DFPTraining\n",
    "from dfp.stages.multi_file_source import MultiFileSource\n",
    "from dfp.stages.s3_object_source_stage import S3BucketSourceStage\n",
    "from dfp.stages.s3_object_source_stage import s3_filter_duo\n",
    "from dfp.stages.s3_object_source_stage import s3_object_generator\n",
    "from dfp.stages.write_to_s3_stage import WriteToS3Stage\n",
    "\n",
    "# Create a linear pipeline object\n",
    "pipeline = LinearPipeline(config)\n",
    "\n",
    "# Source stage uses \n",
    "pipeline.set_source(\n",
    "    MultiFileSource(config,\n",
    "                    input_schema=input_schema,\n",
    "                    filenames=[\"/work/examples/data/dfp/duo/duotest_pt1.json\", \"/work/examples/data/dfp/duo/duotest_pt2.json\", \"/work/examples/data/dfp/duo/duotest_pt3.json\", \"/work/examples/data/dfp/duo/duotest_pt4.json\"],\n",
    "                    parser_kwargs={\n",
    "                        \"lines\": False, \"orient\": \"records\"\n",
    "                    }))\n",
    "\n",
    "# This will split users or just use one single user\n",
    "pipeline.add_stage(\n",
    "    DFPSplitUsersStage(config,\n",
    "                       include_generic=include_generic,\n",
    "                       include_individual=include_individual,\n",
    "                       skip_users=skip_users))\n",
    "\n",
    "# Next, have a stage that will create rolling windows\n",
    "pipeline.add_stage(\n",
    "    DFPRollingWindowStage(\n",
    "        config,\n",
    "        min_history=300 if is_training else 1,\n",
    "        min_increment=300 if is_training else 1,\n",
    "        # For inference, we only ever want 1 day max\n",
    "        max_history=\"5d\" if is_training else \"1d\",\n",
    "        cache_dir=cache_dir))\n",
    "\n",
    "# Specify the final set of columns necessary just before pre-processing\n",
    "model_column_info = [\n",
    "    # Input columns\n",
    "    RenameColumn(name=\"accessdevicebrowser\", dtype=str, input_name=\"accessdevicebrowser\"),\n",
    "    RenameColumn(name=\"accessdeviceos\", dtype=str, input_name=\"accessdeviceos\"),\n",
    "    RenameColumn(name=\"device\", dtype=str, input_name=\"device\"),\n",
    "    RenameColumn(name=\"result\", dtype=bool, input_name=\"result\"),\n",
    "    RenameColumn(name=\"reason\", dtype=str, input_name=\"reason\"),\n",
    "    # Derived columns\n",
    "    CustomColumn(name=\"logcount\", dtype=int, process_column_fn=column_logcount),\n",
    "    CustomColumn(name=\"locincrement\", dtype=int, process_column_fn=column_locincrement),\n",
    "    # Extra columns\n",
    "    RenameColumn(name=\"username\", dtype=str, input_name=\"username\"),\n",
    "    RenameColumn(name=config.ae.timestamp_column_name, dtype=datetime, input_name=config.ae.timestamp_column_name),\n",
    "]\n",
    "\n",
    "model_schema = DataFrameInputSchema(column_info=model_column_info, preserve_columns=[\"_batch_id\"])\n",
    "\n",
    "# Output is UserMessageMeta -- Cached frame set\n",
    "pipeline.add_stage(DFPPreprocessingStage(config, input_schema=model_schema, only_last_batch=not is_training))\n",
    "\n",
    "if (is_training):\n",
    "\n",
    "    pipeline.add_stage(DFPTraining(config))\n",
    "\n",
    "    pipeline.add_stage(DFPMLFlowModelWriterStage(config))\n",
    "else:\n",
    "    pipeline.add_stage(DFPInferenceStage(config))\n",
    "\n",
    "    pipeline.add_stage(DFPPostprocessingStage(config))\n",
    "\n",
    "    pipeline.add_stage(WriteToFileStage(config, filename=\"dfp_detections.csv\", overwrite=True))\n",
    "\n",
    "# Run the pipeline\n",
    "await pipeline._do_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c848c1-d56d-42a0-83ad-19d6fe9731a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430831c-03c0-4cfb-8688-8777870ca722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:morpheus] *",
   "language": "python",
   "name": "conda-env-morpheus-py"
  },
  "vscode": {
   "interpreter": {
    "hash": "e26783b24f020aa0bcaa00e6ba122db5d0e3da2d892d80be664969895e06a7e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
